---
title: "PP-TFIDF"
author: "Perilium"
date: "2018年10月17日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 載入資料庫

```{r library, warning=FALSE}
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
```
```{r gatdata}
rawData = readtext("C:\\Users\\perot\\Desktop\\NTU-CSX4001\\Week_5\\hw_5\\TFIDF_PrivacyPolicy\\corpus\\*.txt")
docs = Corpus(VectorSource(rawData$text))
```
## Including Plots

```{r clean, warning=FALSE}
for (j in seq(docs)){
  for(i in 1:length(keywords$X1)){
  docs[[j]] <- gsub(keywords$X1[i],keywords$X2[i], docs[[j]])
  }
}
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, content_transformer(tolower))
docsPTD <- tm_map(docs, PlainTextDocument)
keywords <- read.csv("keywords.csv")
str(docs)
keywords = read.csv("keywords.csv")
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)

jieba_tokenizer = function(d){
  unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
```


```{r TF}
freqFrame = as.data.frame(table(unlist(seg)))

d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)

print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
```


```{r IDF}
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{ 
  log2( N / nnzero(word_doc) ) 
}
idf <- apply(tdm, 1, idfCal)

doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
  for(y in 1:ncol(tdm))
  {
    doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
  }
}

findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
write.csv(tfidfnn, "show.csv")
```
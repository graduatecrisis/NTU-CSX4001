leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel <-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel <-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(leafletCN)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel <- as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## 載入資料庫
```{r inside}
library(leaflet)
library(leafletCN)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat
dat$Tel <- as.character(dat$Tel)
dat
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat
dat$Num <- as.factor(dat$Tel)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Num <- as.factor(dat$Num)
dat
for
dat
dat
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
dat
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"\n電話:",dat$Tel[i])
}
dat$Num <- as.factor(dat$Num)
dat
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"<\n>電話:",dat$Tel[i])
}
dat$Num <- as.factor(dat$Num)
dat
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i])
}
dat
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
map<-  leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
map
View(map)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(leafletCN)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat
dat$Num <- as.factor(dat$Num)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel)
```{r map}
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
unlink('C:/Users/perot/Desktop/NTU-CSX4001/Week_4/hw_4/map/Hsinchu_7-11_cache', recursive = TRUE)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Name, label = ~Name)
library(leaflet)
library(htmltools)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Name, label = ~Name)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i],sep="")
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"  電話:",dat$Tel[i],sep="")
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
plot(iris)
barplot(iris)
library(ggmap)
library(mapproj)
map <- get_map(location = 'Taiwan', zoom = 7)
has_goog_key()
goog_key()
has_goog_client()
has_goog_signature()
register_google(key = "AIzaSyB8N5qP3TmqFQisSDHaBPMu_7rWGyXf0Ec")
map <- get_map(location = 'Taiwan', zoom = 7)
has_goog_key()
goog_key()
has_goog_client()
has_goog_signature()
register_google(key = "AIzaSyB8N5qP3TmqFQisSDHaBPMu_7rWGyXf0Ec")
map <- get_map(location = 'Taiwan', zoom = 7)
ggmap(map)
map <- get_map(location = 'Taiwan', zoom = 7)
help
clean
cls
help
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
install.package("proxy")
install.packages("proxy")
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
install.packages("readtext")
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
rawData = readtext("*.txt")
setwd("C:/Users/perot/Desktop/107-1RSampleCode/week_5/task_5")
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
# words cut
keywords = read.csv("keywords.csv")
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
write.csv(tfidfnn, "show.csv")
rawData = readtext("*.txt")
View(rawData)
linrary(readr)
library(readr)
knitr::knit('Wordcloud_Tweet_Indertale.Rmd',encoding = 'UTF8')
unlink('C:/Users/perot/Desktop/NTU-CSX4001/Week_4/hw_4/Wordcloud/Wordcloud_Tweet_Undertale_jp_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::knit('Wordcloud_Tweet_Indertale.Rmd',encoding = 'UTF8')
knitr::knit(encoding = 'UTF8')
setwd("C:/Users/perot/Desktop/NTU-CSX4001/Week_4/hw_4/Wordcloud")
options(encoding = 'UTF-8')
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(encoding = 'UTF-8')
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
library(ggplot2)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#Remove other nonsense words
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","??","1","de"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#do a word count
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","??","1","de",'さん','した'))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#Remove other nonsense words
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","??","1","de",'さん','した'))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#do a word count
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
write.csv("raw")
write.csv(hmt,raw)
write.csv(hmt, "raw.txt")
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
write.csv(hmt, "raw.txt")
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
write.csv(hmtTable, "raw.txt")
write.csv(hmtTable, "raw.txt")
hmtTable <- hmtTable %>%
anti_join(stop_words)
#Remove other nonsense words
write.csv(hmtTable, "raw.txt")
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
write.csv(hmtTable, "raw.txt")
#Remove other nonsense words
write.csv(hmtTable, "raw.csv")
#Remove other nonsense words
#write.csv(hmtTable, "raw.csv")
hmtTable <- read.csv("raw.csv")
View(hmtTable)
knitr::opts_chunk$set(echo = TRUE)
options(encoding = 'UTF-8')
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
library(ggplot2)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
#hmt <- search_tweets(
#  "#undertale", n = 10000, include_rts = FALSE
#)
#Unnest the words - code via Tidy Text
#hmtTable <- hmt %>%
#  unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
#data(stop_words)
# hmtTable <- hmtTable %>%
#  anti_join(stop_words)
# hmtTable <- hmtTable %>%
#  count(word, sort = TRUE)
# hmtTable
#Remove other nonsense words
#write.csv(hmtTable, "raw.csv")
hmtTable <- read.csv("raw.csv")
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","1","de",'さん','した'))
#do a word count
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
#write.csv(hmtTable, "raw.csv")
hmtTable <- read.csv("raw.csv")
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","1","de",'さん','した'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
#write.csv(hmtTable, "raw.csv")
hmtTable <- read.csv("raw.csv",row.name = 1)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","1","de",'さん','した'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","1","de",'さん','した','<U+306E>'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
library(ggplot2)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
#hmt <- search_tweets(
#  "#undertale", n = 10000, include_rts = FALSE
#)
#Unnest the words - code via Tidy Text
#hmtTable <- hmt %>%
#  unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
#data(stop_words)
# hmtTable <- hmtTable %>%
#  anti_join(stop_words)
# hmtTable <- hmtTable %>%
#  count(word, sort = TRUE)
# hmtTable
#Remove other nonsense words
#write.csv(hmtTable, "raw.csv")
hmtTable <- read.csv("raw.csv",row.name = 1)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","1","de",'さん','した','<U+306E>'))
#do a word count
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
View(hmtTable)
library("wordcloud")
library("RColorBrewer")
library(dplyr)
library(rtweet)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
library(ggplot2)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
#write.csv(hmtTable, "raw.csv")
#hmtTable <- read.csv("raw.csv",row.name = 1)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","?`","3","day","1","de",'さん','した','<U+306E>'))
#do a word count
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))

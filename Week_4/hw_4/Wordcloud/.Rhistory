for (i in c(1:135)){
map<- add.dot(data$緯度[i],data$經度[i])
}
map
leaflet(data = data[1:20,]) %>% addTiles() %>%
addMarkers(~long, ~lat, popup = ~as.character(mag), label = ~as.character(mag))
leaflet(data = data[1:20,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度, popup = ~as.character(mag), label = ~as.character(mag))
leaflet(data = data[1:20,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
leaflet(data = data[1:135]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
leaflet(data = data[1:135]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
View(data)
leaflet(data = data[1:20]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
leaflet(data = data[20:2]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
leaflet(data = data[1:135,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
leaflet(data = data[1:135,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
library(leaflet)
data <- read.csv("C:\\Users\\perot\\Desktop\\NTU-CSX4001\\Week_4\\hw_4\\map\\address.csv")
data <- data.frame(data)
leaflet(data = data[1:135,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
library(leaflet)
data <- read.csv("C:\\Users\\perot\\Desktop\\NTU-CSX4001\\Week_4\\hw_4\\map\\address.csv")
leaflet(data = data[1:135,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
## 載入資料庫
```{r setup}
library(leaflet)
data <- read.csv("C:\\Users\\perot\\Desktop\\NTU-CSX4001\\Week_4\\hw_4\\map\\address.csv")
leaflet(data = data[1:135,]) %>% addTiles() %>%
addMarkers(~經度, ~緯度)
setwd("C:/Users/perot/Desktop/NTU-CSX4001/Week_4/hw_4/Textcloud")
install.packages("jiebaR")
install.packages("rtweet")
install.packages("tidytext")
install.packages("dplyr")
install.packages("stringr")
require(devtools)
library(tidytext)
library(dplyr)
install.packages("tidytext")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(wordcloud2)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets - note: reduce to 1000 if it's slow
hmt <- search_tweets(
"#undertale", n = 2000, include_rts = FALSE
)
hmt$text
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
View(hmtTable)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ'))
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
wordcloud2(hmtTable, size=0.7)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり'))
wordcloud2(hmtTable, size=0.7)
hmtTable
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です',
'だ'))
wordcloud2(hmtTable, size=0.7)
hmtTable
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です',
'だ','2','も','れ'))
wordcloud2(hmtTable, size=0.7)
hmtTable
View(hmtTable)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です',
'だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>'))
wordcloud2(hmtTable, size=1)
hmtTable
wordcloud2(hmtTable, size=1,min.freq = 7)
wordcloud(words = d$word, freq = d$n, min.freq = 10,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
install.packages("tm")  # for text mining
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
# Load
library("wordcloud")
library("RColorBrewer")
wordcloud(words = d$word, freq = d$n, min.freq = 10,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 10,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 10,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 21,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 21,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 21,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です',
'だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん',
'かな','けど','しま',))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 21,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です',
'だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん',
'かな','けど','しま'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 21,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co', 'https', 'undertale', "art", "it's", 'el', 'en', 'tv','に','た','が','て','の','と','は',
'で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です',
'だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん',
'かな','けど','しま','ま','よ'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 21,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
jpstop
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","<U+3063><U+3066>"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
jpstop
jpstop <- paste(jpsyop,c("alpha"))
jpstop <- paste(jpsyop,"alpha")
hmtTable
View(hmtTable)
#Remove other nonsense words
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","<U+3063><U+3066>"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","<U+3063><U+3066>","<U+3063>
<U+3063><U+3066>","<U+3057><U+305F>","<U+30C6><U+30FC><U+30EB>","<U+30A2><U+30F3><U+30C0><U+30FC>","<U+305F><U+3044>"
))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","<U+3063><U+3066>","<U+3063>
<U+3063><U+3066>","<U+3057><U+305F>","<U+30C6><U+30FC><U+30EB>","<U+30A2><U+30F3><U+30C0><U+30FC>","<U+305F><U+3044>"
))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
View(hmtTable)
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","<U+3063>","<U+3063><U+3066>","<U+3063>
<U+3063><U+3066>","<U+3057><U+305F>","<U+30C6><U+30FC><U+30EB>","<U+30A2><U+30F3><U+30C0><U+30FC>","<U+305F><U+3044>"
))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1"))
hmtTable
library(plotly)
plot_ly(mydata, x = ~word, y = ~n, type = "histogram")
plot_ly(hmtTable, x = ~word, y = ~n, type = "histogram")
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
plot_ly(hmtTable, x = ~word, y = ~n, type = "histogram")
plot_ly(hmtTable, x = ~word, type = "histogram")
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de"))
plot_ly(hmtTable, x = ~word, type = "histogram")
plot_ly(hmtTable, x = ~word, type = "bar")
hmtTable
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#do a word count
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
```{r cloud}
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って"))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
library(plotly)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#do a word count
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#do a word count
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
View(stop_words)
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テイル","つ",""))
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した"))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
library(stopwords)
library(plotly)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#do a word count
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した"))
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した","フリ","よ","てい"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
library(ggplot2)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#do a word count
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した","フリ","よ","てい"))
ggplot(hmtTable, aes(x = word)) + geom_bar()
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した","フリ","スク","よ","てい"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した","フリ","スク","よ","てい","ちゃん","アンダ"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した","フリ","スク","よ","てい","ちゃん","アンダ","たら","ね","アンテ"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1","de","っ","って","5","目","み","いい","アンダー","テール","つ","さん","たい","ま","中","一","した","フリ","スク","よ","てい","ちゃん","アンダ","たら","ね","アンテ","けど"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))

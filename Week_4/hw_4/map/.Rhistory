#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>','day'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
hmtTable
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>','day','<U+306E>','<U+305F>','<U+306B>','<U+3068>'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>','day','<U+306E>','<U+305F>','<U+306B>','<U+3068>'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','の','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>','day','<U+306E>','<U+305F>','<U+306B>','<U+3068>'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
hmtTable
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
#hmtTable <-hmtTable %>%
#  filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale','<U+306E>'                     ))
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#Remove other nonsense words
hmtTable <-hmtTable %>%
##  filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
View(hmtTable)
#Remove other nonsense words
hmtTable <-hmtTable %>%filter(!word %in% c(<U+306E>)
#Remove other nonsense words
hmtTable <-hmtTable %>%filter(!word %in% c(<U+306E>))
#Remove other nonsense words
hmtTable <-hmtTable %>%filter(!word %in% c('<U+306E>'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
knitr::opts_chunk$set(echo = TRUE)
library("wordcloud")
library("RColorBrewer")
library(tidytext)
library(dplyr)
library(stringr)
library(rtweet)
create_token(
app = "CSX4001",
consumer_key = "os9BSEDDSzZKaa79hWbXeAUmm",
consumer_secret = "K6KcZy4ZhQYpFoMWKKgZjVTXKSP4gAvTc760meWwmlQLJEWsAg",
access_token = "3301954536-dvRxov51hUw3tU8yrJJnIDGHfhL0gVimjGX92u6",
access_secret = "4Y7uewr8SyVmdAXuFBp2jXKskaLVuasmtPoizyR2Fx2x5",
)
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','に','た','が','て','の','と','は','で','を','って','い','な','っ','し','した','たい','アンダー','テール','繋がり','さん','ない','か','です','だ','2','も','れ','描','<U+307E>','<U+307E><U+3059>','<U+307F>','3','<U+304B><U+3089>',"ます",'や','もう','ちゃん','かな','けど','しま','ま','よ','day','み','から','見'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
stop_words
open(stop_words)
stop_words
View(stop_words)
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','って','っ','した','たい','アンダー','テール','繋がり','さん','ない','です','2','描','3',"ます",'もう','ちゃん','かな','けど','しま','day','から','見','あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と','な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ','ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ','だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ','きゃ','きゅ','きょ','しゃ','しゅ','しょ','ちゃ','ちゅ','ちょ','にゃ','にゅ','にょ','ひゃ','ひゅ','ひょ','みゃ','みゅ','みょ','りゃ','りゅ','りょ'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
hmtTable <-hmtTable %>%
filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','って','っ','した','たい','アンダー','テール','繋がり','さん','ない','です','2','描','3',"ます",'もう','ちゃん','かな','けど','しま','day','から','見','あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と','な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ','ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ','だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ','きゃ','きゅ','きょ','しゃ','しゅ','しょ','ちゃ','ちゅ','ちょ','にゃ','にゅ','にょ','ひゃ','ひゅ','ひょ','みゃ','みゅ','みょ','りゃ','りゅ','りょ','版','ゅ'))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
install.packages("stopwords")
library(stopwords)
hmtTable <- hmtTable %>%
anti_join(stop_words) %>%
anti_join(stopwords::stopwords("jp", source = "stopwords-iso"))
hmtTable <- hmtTable %>%
anti_join(stop_words) %>%
anti_join(stopwords::stopwords("ja", source = "stopwords-iso"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <- hmtTable %>%
anti_join(stop_words) %>%
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#Remove other nonsense words
hmtTable <-hmtTable %>%
#  filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','って','っ','した','たい','アンダー','テール','繋がり','さん','ない','です','2','描','3',"ます",'もう','ちゃん','かな','けど','しま','day','から','見','あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と','な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ','ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ','だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ','きゃ','きゅ','きょ','しゃ','しゅ','しょ','ちゃ','ちゅ','ちょ','にゃ','にゅ','にょ','ひゃ','ひゅ','ひょ','みゃ','みゅ','みょ','りゃ','りゅ','りょ','版','ゅ'))
filter(!word %in%  jpstop)
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
```{r main}
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
hmtTable <- hmtTable %>%
anti_join(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
#Remove other nonsense words
hmtTable <-hmtTable %>%
#  filter(!word %in% c('t.co','https','undertale',"art","it's",'el','en','tv','って','っ','した','たい','アンダー','テール','繋がり','さん','ない','です','2','描','3',"ます",'もう','ちゃん','かな','けど','しま','day','から','見',))
filter(!word %in%  jpstop)
hmtTable
#Remove other nonsense words
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","って","っ","した","たい","アンダー","テール","繋がり","さん","ない","です","2","描","3","ます","もう","ちゃん","かな","けど","しま","day","から","見"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable
#Grab tweets
hmt <- search_tweets(
"#undertale", n = 10000, include_rts = FALSE
)
#Unnest the words - code via Tidy Text
hmtTable <- hmt %>%
unnest_tokens(word, text)
#remove stop words - aka typically very common words such as "the", "of" etc
data(stop_words)
jpstop <- stopwords::stopwords("ja", source = "stopwords-iso")
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#do a word count
hmtTable <- hmtTable %>%
count(word, sort = TRUE)
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
#Remove other nonsense words
hmtTable <-hmtTable %>%  filter(!word %in%  jpstop) %>%
filter(!word %in% c("t.co","https","undertale","art","it's","2","描","3","day","見","1"))
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable <- hmtTable %>%
anti_join(stop_words)
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable
wordcloud(words = hmtTable$word, freq = hmtTable$n, min.freq = 50,
random.order=FALSE, rot.per=0.35,scale = c(3, 1),
colors=brewer.pal(12,"Paired" ))
hmtTable
jpstop
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Num)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Name <- as.character(dat$Name)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Num,label = ~Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,label = ~Tel)
is.character(dat$Tel)
is.vector(dat$Tel)
str(dat$Tel)
dat$Tel <-as.character(dat$Tel)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,label = ~Tel)
addMarkers(~Lon,~Lat, popup = ~Tel,label = paste("Tel:",~Tel,sep="")
leaflet(data = dat[1:134,]) %>% addTiles() %>%
dat$content <- paste("Tel:",~Tel,sep="")
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,label = ~content)
dat$content[i] <- paste("門市名稱：",dat$Name[i],"\nTel:",dat$Tel[i],sep="")
for(i in c(1:134)){
dat$content[i] <- paste("門市名稱：",dat$Name[i],"\nTel:",dat$Tel[i],sep="")
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,label = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
dat$content[i] <- paste("門市名稱：",dat$Name[i],"Tel:",dat$Tel[i],sep="")
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
dat$content[i] <- paste("門市名稱：",dat$Name[i],"<br/>Tel:",dat$Tel[i],sep="")
for(i in c(1:134)){
dat$content[i] <- paste("門市名稱：",dat$Name[i],"<br/>Tel:",dat$Tel[i],sep="")
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
for(i in c(1:134)){
dat$content[i] <- paste("門市名稱：",dat$Name[i],"<br/>Tel:",dat$Tel[i],sep="")
}
for(i in c(1:134)){
dat$content[i] <- paste("門市名稱：",dat$Name[i],"Tel:",dat$Tel[i],sep="")
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
for(i in c(1:134)){
dat$content[i] <- paste(sep="<br/>",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel <-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste(sep="<br/>",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel <-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel <-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste(sep=":",dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
for(i in c(1:134)){
dat$content[i] <- paste(dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
View(dat)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste(dat$Name[i],dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
setwd("C:/Users/perot/Desktop/NTU-CSX4001/Week_4/hw_4/map")
knitr::opts_chunk$set(echo = TRUE)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
install.packages(leafletCN)
install.packages("leafletCN")
library(leafletCN)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
library(leaflet)
library(leafletCN)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat,, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat,, popup = ~Tel)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat,, popup = content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Name)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel)
dat$Num <- as.factor(dat$Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,~Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,popup = ~Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~Tel,Num)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
dat$Tel<-as.character(dat$Tel)
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
dat$Tel<-as.character(dat$Tel)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel<-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
dat$Tel <- as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
leaflet(data = dat[1:134,]) %>% addTiles() %>%
addMarkers(~Lon,~Lat, popup = ~content)
library(leaflet)
library(htmltools)
dat <- read.csv("https://perilium.github.io/NTU-CSX4001/Week_4/hw_4/map/7-11raw.csv")
dat$Tel<-as.character(dat$Tel)
for(i in c(1:134)){
dat$content[i] <- paste("門市:",dat$Name[i],"電話:",dat$Tel[i])
}
